{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"},{"sourceId":7086302,"sourceType":"datasetVersion","datasetId":4082864}],"dockerImageVersionId":30581,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1) Getting Setup","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!pip install jupyter --upgrade\n!pip install ipywidgets widgetsnbextension --upgrade\n!pip install -q peft","metadata":{"execution":{"iopub.status.busy":"2023-11-30T22:53:48.223213Z","iopub.execute_input":"2023-11-30T22:53:48.223632Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.12)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.34.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport wandb\nimport transformers\nimport torch\nimport glob\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\nimport os\nimport random\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom torch import nn\nimport sys\nimport gc\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AdamW\nfrom accelerate import notebook_launcher\nfrom sklearn.model_selection import train_test_split\nfrom accelerate import DistributedDataParallelKwargs\nimport time\nimport re\nfrom transformers import get_cosine_schedule_with_warmup","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment to enable Fully Sharded Data Parallel\n# os.environ[\"ACCELERATE_USE_FSDP\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From this Gist: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_from_class(cls):\n    return dict((key, value) for (key, value) in cls.__dict__.items() if not \"__\" in key )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    # General Configuration\n    seed = 42\n    base_path = \"/kaggle/input/my-dear-watson-backtranslated-dataset\"\n    mode = \"maximize\"\n    device_type = \"gpus\"\n    \n    # WandB Configuration\n    name = \"Multilingual Models\"\n#     model_name = \"facebook/xlm-roberta-xl\"\n#     model_name = \"bert-base-multilingual-cased\"\n    model_name = \"xlm-roberta-large\"\n    metric_name = \"accuracy\"\n\n    # Training Hyperparameters\n    lr = 1e-4\n    epochs = 40\n    patience = 8\n    grad_accum = 4\n    grad_norm = 1.0           # Gradient Clipping\n    optimizer = \"AdamW\"\n    scheduler = \"Cosine\"\n    weight_decay = 0.3\n    pearson_weight = 0.0      # Percent of weight to put onto Pearson Correlation (Do not use due to NAN losses)\n    warmup = 0.1\n    mean_max_sampling = True # Whether or not to use mean-max sampling on the final BERT layer (Don't use due to instability)\n    \n    # Data Configuration\n    truncation = True\n    padding = True\n    test_size = 0.2\n    back_translate = 0.5      # Percent of time to back translate\n    upsample = False          # Whether or not to upsample\n    \n    # LoRA hyperparameters\n    r = 8\n    lora_alpha = 16\n    lora_dropout = 0.0        # Dropout on LoRA Layers (Do not use as transformer already has dropout on by default)\n    bias = \"all\"\n        \nconfig.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\nconfig.checkpoint = f\"/kaggle/working/{config.model_name}.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config.device_type == \"gpu\":\n    config.batch_size = 8\nelif config.device_type == \"gpus\":\n    config.batch_size = 8\nelif config.device_type == \"tpu\":\n    # Batch of 128 for each TPU core\n    config.batch_size = 128","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Signing into WandB\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\n!wandb login $secret_value_0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1) Trackers","metadata":{}},{"cell_type":"code","source":"class LossTracker:\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AccuracyTracker():\n    def __init__(self):\n        self.correct_predictions = 0.0\n        self.total_predictions = 0.0\n        \n    def update(self, y_hat, y):\n        preds = y_hat.detach().cpu().numpy()\n        labels = y.detach().cpu().numpy()\n        \n        n = len(preds)\n        self.correct_predictions += (preds == labels).sum()        \n        self.total_predictions += n\n    \n    def score(self):\n        return self.correct_predictions / self.total_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelTracker():\n    def __init__(self, model, optimizer, scheduler, accelerator):\n        self.missed = 0\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.metric = float(\"-inf\") if config.mode == \"maximize\" else float(\"inf\")\n        self.accelerator = accelerator\n        \n    def save_helper(self, epoch):\n        self.accelerator.save({\n                    \"epoch\": epoch, \n                    \"model_state_dict\": self.accelerator.unwrap_model(self.model).state_dict(), \n                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n                    \"scheduler\": self.scheduler.state_dict()\n                }, config.checkpoint)\n\n        self.accelerator.print(f\"Saved to model to {config.checkpoint}!\")\n        \n    def save_model(self, epoch):\n        self.save_helper(epoch)\n        \n\n    def update(self, value, epoch):\n        if config.mode == \"maximize\":\n            if value >= self.metric:\n                self.accelerator.print(f\"Validation {config.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n                self.metric = value\n                self.save_model(epoch)    \n                self.missed = 0\n\n            else:\n                self.accelerator.print(f\"Validation {config.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n                self.accelerator.print(f\"Model did not improve on epoch {epoch}\")\n                self.missed += 1\n        else:\n            if value <= self.metric:\n                self.accelerator.print(f\"Validation {config.metric_name} fell from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n                self.metric = value\n                self.save_model(epoch) \n                self.missed = 0\n\n            else:\n                self.accelerator.print(f\"Validation {config.metric_name} rose from {self.metric:.4f} to {value:.4f} on epoch {epoch}\")\n                self.accelerator.print(f\"Model did not improve on epoch {epoch}\")\n                self.missed += 1\n        \n    def check_improvement(self):\n        return self.missed < config.patience","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Data Loading","metadata":{}},{"cell_type":"markdown","source":"# 2.1) Data Loading","metadata":{}},{"cell_type":"code","source":"class TrainData(Dataset):\n    def __init__(self, df):\n        self.df = df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        selection = self.df.iloc[index]\n    \n        # Back Translation Code\n        premise = selection[\"bt_premise\"] if np.random.uniform() <= config.back_translate else selection[\"premise\"]\n        hypothesis = selection[\"bt_hypothesis\"] if np.random.uniform() <= config.back_translate else selection[\"hypothesis\"]\n        \n        return premise, hypothesis, selection[\"label\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestData(Dataset):\n    def __init__(self, df):\n        self.df = df\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        selection = self.df.iloc[index]\n        return selection[\"premise\"], selection[\"hypothesis\"], selection[\"label\"], selection[\"language\"]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turns each batch of data into tensors\ndef train_collate_dynamic_padding(batch):\n    batch = np.array(batch, dtype = \"object\")\n    text_input = batch[:, 0:2].tolist()\n    labels = batch[:, 2].astype(int)\n    \n    tokens = config.tokenizer(text_input, padding=config.padding, truncation = config.truncation, return_tensors=\"pt\")\n    return tokens, torch.tensor(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turns each batch of data into tensors\ndef test_collate_dynamic_padding(batch):\n    batch = np.array(batch, dtype = \"object\")\n    text_input = batch[:, 0:2].tolist()\n    labels = batch[:, 2].astype(int)\n    languages = batch[:, 3]\n    \n    tokens = config.tokenizer(text_input, padding=config.padding, truncation = config.truncation, return_tensors=\"pt\")\n    return tokens, torch.tensor(labels), languages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, train_len):\n        super(Model, self).__init__()\n        self.train_len = train_len\n        self.base_model = AutoModel.from_pretrained(config.model_name)\n        if config.mean_max_sampling:\n            self.fc = nn.Linear(2 * self.base_model.config.hidden_size, 3)\n        else:\n            self.fc = nn.Linear(self.base_model.config.hidden_size, 3)\n    \n    def feature(self, inputs):\n        features = self.base_model(**inputs)[\"last_hidden_state\"]\n        # Taking the mean and max of all last hidden state tokens\n        if config.mean_max_sampling:\n            mean_pooling_embeddings = torch.mean(features, 1)\n            _, max_pooling_embeddings = torch.max(features, 1)\n            mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n\n            return mean_max_embeddings\n        else:\n            return features[:, 0, :]\n    \n    def forward(self, inputs):\n        features = self.feature(inputs)\n        \n        return self.fc(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Training Loop","metadata":{}},{"cell_type":"markdown","source":"## 4.1) Data Preparation","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(f\"{config.base_path}/train.csv\")\ntrain, test = train_test_split(train, test_size = config.test_size, stratify = train[\"lang_abv\"], random_state = config.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Data and Test Data are both the same in this case!\n# They are distinguished for logging purposes!\ntrain_data, val_data, test_data = TrainData(train), TrainData(test), TestData(test)\n\nif config.upsample:\n# Getting sample weights for balancing data\n    weights = (1 / train.language.value_counts()).to_dict()\n    train[\"weight\"] = train.apply(lambda row: weights[row.language], axis = 1)\n    sample_weights = list(train[\"weight\"])\n\n    train_sampler = WeightedRandomSampler(sample_weights, len(train_data))\n\n    train_data_loader = DataLoader(train_data, collate_fn = train_collate_dynamic_padding, batch_size = config.batch_size, pin_memory = True, num_workers = os.cpu_count(), sampler = train_sampler)\n    \nelse:\n    train_data_loader = DataLoader(train_data, collate_fn = train_collate_dynamic_padding, batch_size = config.batch_size, pin_memory = True, num_workers = os.cpu_count(), shuffle = True)\n\n\nval_data_loader = DataLoader(test_data, collate_fn = train_collate_dynamic_padding, batch_size = config.batch_size, pin_memory = True, num_workers = os.cpu_count())\ntest_data_loader = DataLoader(test_data, collate_fn = test_collate_dynamic_padding, batch_size = config.batch_size, pin_memory = True, num_workers = os.cpu_count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2) Training Functions","metadata":{}},{"cell_type":"code","source":"def generateConfusionMatrices(val_preds):\n    for lang in val_preds.langs.unique():\n        filtered_df = val_preds.loc[val_preds.langs == lang]\n        y = list(filtered_df.y.astype(int))\n        y_hat = list(filtered_df.y_hat.astype(int))\n        \n        wandb.log({f\"{lang} Confusion Matrix\": wandb.plot.confusion_matrix(y_true=y, preds=y_hat, class_names=[\"entailment\", \"contradiction\", \"neutral\"], title = f\"{lang} Confusion Matrix\", )})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, accelerator):\n    model.train()\n    model.to(config.device)\n    \n    loss_tracker = LossTracker()\n    accuracy_tracker = AccuracyTracker()\n\n    progress_bar = tqdm(train_loader, desc = f\"Training Loop Epoch: {epoch}\")\n    \n    average_acc = None\n    \n    for batch_idx, batch in enumerate(progress_bar):\n        with accelerator.accumulate(model):\n            inputs, labels = batch\n\n            for key, value in inputs.items():\n                inputs[key] = value.to(config.device)\n\n            labels = labels.to(config.device)\n            batch_size = labels.size(0)\n\n            logits = model(inputs)\n            # PyTorch CrossEntropy uses the unnormalized logits\n            train_loss = criterion(logits, labels)\n            scaled_loss = train_loss / config.grad_accum\n\n            y_hat = torch.nn.functional.softmax(logits, dim = 1)\n            y_hat = y_hat.argmax(dim = 1)\n\n            accuracy_tracker.update(y_hat, labels)\n\n            loss_tracker.update(train_loss.item(), batch_size)\n\n            accelerator.backward(scaled_loss)\n\n            if ((batch_idx + 1) % config.grad_accum == 0) or (batch_idx + 1 == len(train_loader)):\n\n                # Clip gradients once all of them are synced to main process\n                if accelerator.sync_gradients:\n                    accelerator.clip_grad_norm_(model.parameters(), config.grad_norm)\n                    \n                optimizer.step()\n\n                optimizer.zero_grad()\n\n                if not scheduler is None:\n                    scheduler.step()\n                    for i, lr in enumerate(scheduler.get_last_lr()):\n                        accelerator.log({f\"Layer {i} Learning Rate\": lr})\n                        \n                avg_accuracy = accuracy_tracker.score()\n                avg_loss = loss_tracker.avg\n                step_loss = loss_tracker.val\n                learning_rate = scheduler.get_last_lr()[0]\n\n                text = f\"Epoch: {epoch} | Average Training Accuracy: {avg_accuracy:.4f} | Average Training Loss: {avg_loss:.4f} | Step Training Loss: {step_loss:.4f} | Learning Rate: {learning_rate:.4f}\"\n                progress_bar.set_postfix_str(text)\n                progress_bar.refresh()\n\n                accelerator.log({f\"Step Training Loss\": step_loss})\n                \n\n    epoch_loss = loss_tracker.avg\n    epoch_accuracy = accuracy_tracker.score()\n\n    accelerator.log({f\"Training Loss Epoch\": epoch_loss})\n    accelerator.log({f\"Training Accuracy Epoch\": epoch_accuracy})\n    accelerator.print(f\"Training Loss: {epoch_loss} | Training Accuracy: {epoch_accuracy}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_fn(val_loader, model, criterion, epoch, accelerator):\n    with torch.no_grad():\n        model.eval()\n        model.to(config.device)\n\n        loss_tracker = LossTracker()\n        accuracy_tracker = AccuracyTracker()\n\n        progress_bar = tqdm(val_loader, desc = f\"Validation Loop Epoch: {epoch}\")\n\n        for batch_idx, batch in enumerate(progress_bar):\n\n            inputs, labels = batch\n\n            for key, value in inputs.items():\n                inputs[key] = value.to(config.device)\n\n            labels = labels.to(config.device)\n            batch_size = labels.size(0)\n\n            logits = accelerator.unwrap_model(model)(inputs)\n            y_hat = torch.nn.functional.softmax(logits, dim = 1)\n            y_hat = y_hat.argmax(dim = 1)\n\n            val_loss = criterion(logits, labels)\n\n            accuracy_tracker.update(y_hat, labels)\n            loss_tracker.update(val_loss.item(), batch_size)\n\n            avg_val_loss = loss_tracker.avg\n            avg_val_acc = accuracy_tracker.score()\n\n            progress_bar.set_postfix_str(f\"Epoch: {epoch} | Average Validation Accuracy {avg_val_acc:.4f}| Average Validation Loss: {avg_val_loss:.4f}\")\n            progress_bar.refresh()\n\n\n        epoch_loss = loss_tracker.avg\n        epoch_accuracy = accuracy_tracker.score()\n\n        accelerator.log({f\"Validation Loss Epoch\": epoch_loss})\n        accelerator.log({f\"Validation Accuracy Epoch\": epoch_accuracy})\n        accelerator.print(f\"Validation Loss: {epoch_loss} | Validation Accuracy: {epoch_accuracy}\")\n    \n    return epoch_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_fn(test_loader, model, checkpoint, accelerator):\n    with torch.no_grad():\n        model.to(accelerator.device)\n        model.eval()\n\n        accuracy_tracker = AccuracyTracker()\n\n        preds = []\n        actual = []\n        langs = []\n\n        progress_bar = tqdm(test_loader, desc = f\"Test Loop\")\n        for batch_idx, batch in enumerate(progress_bar):\n\n            inputs, labels, languages = batch\n\n            for key, value in inputs.items():\n                inputs[key] = value.to(accelerator.device)\n\n            labels = labels.to(acclerator.device)\n            batch_size = labels.size(0)\n\n            logits = model(inputs)\n            y_hat = torch.nn.functional.softmax(logits, dim = 1)\n            y_hat = y_hat.argmax(dim = 1)\n            accuracy_tracker.update(y_hat, labels)\n\n            preds.extend(y_hat.detach().cpu().numpy())\n            actual.extend(labels.detach().cpu().numpy())\n            langs.extend(languages)\n\n\n            avg_val_acc = accuracy_tracker.score()\n\n            progress_bar.set_postfix_str(f\"Average Test Accuracy {avg_val_acc}\")\n            progress_bar.refresh()\n\n        test_accuracy = accuracy_tracker.score()\n\n        wandb.log({f\"Final Test Accuracy\": test_accuracy})\n        accelerator.print(f\"Test Accuracy: {test_accuracy}\")\n\n        test_preds = pd.DataFrame(np.array([preds, actual, langs]).T, columns = [\"y_hat\", \"y\", \"langs\"])\n        test_preds.to_csv(\"Test Predictions.csv\")\n        \n        # I need to run this because accelerator's API doesn't expose anything to directly save a csv.\n        wandb.save(f\"/kaggle/working/Test Predictions.csv\")\n        accelerator.print(\"Saved Test Predictions to /kaggle/working/Test Predictions.csv\")\n\n        generateConfusionMatrices(test_preds)\n\n        return test_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getCustomLoss():\n    def customLoss(output, target):\n        ce = nn.CrossEntropyLoss()\n        x = output.argmax(dim = 1).to(dtype = float)\n        y = target.to(dtype = float)\n\n        vx = x - torch.mean(x)\n        vy = y - torch.mean(y)\n\n        pearsonCost = 1.0 - torch.sum(vx * vy) / (torch.norm(vx) * torch.norm(vy) + 1e-14)\n        ceCost = ce(output, target)\n\n        return ceCost + pearsonCost * config.pearson_weight\n    return customLoss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3) Model Loading","metadata":{}},{"cell_type":"code","source":"# model = torch.compile(Model(len(train_data_loader)))\nunwrapped_model = Model(len(train_data_loader))\n# model = Model(len(train_data_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_modules = str(unwrapped_model.modules)\npattern = r'\\((\\w+)\\): Linear'\nlinear_layer_names = re.findall(pattern, model_modules)\n\nnames = []\n# Print the names of the Linear layers\nfor name in linear_layer_names:\n    names.append(name)\ntarget_modules = list(set(names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(target_modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(r = config.r, lora_alpha=config.lora_alpha, bias = config.bias, lora_dropout = config.lora_dropout, target_modules=target_modules)\nmodel = get_peft_model(unwrapped_model, lora_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.4) Train Loop","metadata":{}},{"cell_type":"code","source":"def train_loop(model):\n    seed_everything(config.seed)\n    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n    if config.device_type == \"gpu\" or config.device_type == \"gpus\":\n        accelerator = Accelerator(mixed_precision = \"fp16\", gradient_accumulation_steps = config.grad_accum, log_with = \"wandb\", kwargs_handlers=[ddp_kwargs])\n        config.device = accelerator.device\n    elif config.device_type == \"tpu\":\n        accelerator = Accelerator(mixed_precision = \"bf16\", gradient_accumulation_steps = config.grad_accum, log_with = \"wandb\", kwargs_handlers=[ddp_kwargs])\n        config.device = accelerator.device\n\n    accelerator.init_trackers(\n        \"My Dear Watson\",\n        config=dict_from_class(config),\n        init_kwargs={\n            \"wandb\": {\n                \"group\": config.name,\n                \"reinit\": False,\n                \"job_type\": config.model_name,\n                \"name\": f\"Seed {config.seed}\",\n                \"entity\": \"uw-kaggle\",\n            }\n        },\n    )\n    ######################################################################\n    criterion = getCustomLoss()\n    optimizer = AdamW(model.parameters(), weight_decay = config.weight_decay, lr = config.lr, correct_bias = True)\n\n    num_training_steps = model.train_len * config.epochs // config.grad_accum\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = num_training_steps * config.warmup, num_training_steps = num_training_steps)\n    model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(model, optimizer, train_data_loader, val_data_loader, scheduler)\n    ######################################################################\n    tracker = ModelTracker(model, optimizer, scheduler, accelerator)\n    \n    for epoch in range(config.epochs):\n\n        train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, accelerator)\n\n        val_accuracy = valid_fn(val_loader, model, criterion, epoch, accelerator)\n\n        accelerator.wait_for_everyone()\n        tracker.update(val_accuracy, epoch)\n\n        if not tracker.check_improvement():\n            print(f\"Stopping the model at epoch {epoch} since the model did not improve!\")\n            break\n    \n    accelerator.wait_for_everyone()\n\n    gc.collect()\n\n    torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if config.device_type == \"tpu\":\n    # Ignore print message that says it's running on 8 GPUs\n    notebook_launcher(train_loop, (model,), num_processes = 8)\n    \nelif config.device_type == \"gpus\":\n    notebook_launcher(train_loop, (model,), num_processes = torch.cuda.device_count())\n    \nelse:\n    train_loop(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%debug","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5) Getting Test Predictions","metadata":{}},{"cell_type":"code","source":"accelerator = Accelerator(log_with = \"wandb\")\naccelerator.init_trackers(\n        \"My Dear Watson\",\n        config=dict_from_class(config),\n        init_kwargs={\n            \"wandb\": {\n                \"group\": config.name,\n                \"reinit\": False,\n                \"resume\": True,\n                \"job_type\": config.model_name,\n                \"name\": f\"Seed {config.seed}\",\n                \"entity\": \"uw-kaggle\",\n            }\n        },\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved = torch.load(config.checkpoint)\n# model = accelerator.unwrap_model(model)\nmodel.load_state_dict(saved[\"model_state_dict\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, test_loader = accelerator.prepare(model, test_data_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_fn(test_loader, model, config.checkpoint, accelerator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accelerator.end_training()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}